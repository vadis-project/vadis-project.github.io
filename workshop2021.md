# VADIS Kickoff Workshop

The kickoff workshop is planned to be on two afternoons in **September 23-24, 2021**. 

We ask all participants of the workshop to register.
Please register via the **[form](https://forms.gle/6CTYWu8P1PZ4zaJW6)**. 
The Zoom link will be sent before the workshop.

## Schedule of the workshop
All times are Central European Summer Time (CEST).

Day 1 (Sept. 23)
* * * * *
	3pm: Welcome, opening remarks
	3:15pm: Project presentation
	3:30-3:55pm: Short break
	4:00pm: Keynote by Arman Cohan: 
	"Facilitating scientific knowledge discovery through improved
	representation learning and extreme summarization"
	4:45-5:00pm: Short break
	5:00pm: Panel 1: Open Issues in Mining Scientific Pubs
	6:00pm End of day 1

Day 2 (Sept. 24)
* * * * *
	3pm: Welcome back
	3:15pm: Panel 2: entity and dataset linking in scientific texts
	4:30-4:45pm: Short break
	4:45pm: Breakout sessions
	5:45pm: Concluding remarks


<hr />


## Keynote

**Title**: Facilitating scientific knowledge discovery through improved representation learning and extreme summarization

**Abstract**: As the pace of scientific publication continues to increase, technologies to help users to search, discover, and understand the scientific literature have become critical. In this talk I will discuss two of our works in this direction that specifically facilitate discovery of relevant scientific information. First I'll present SPECTER, a representation learning model for scientific papers that leverages the citation graph along with the power of Transformers in encoding textual information. SPECTER paper embeddings result in significant improvements in many downstream applications, including recommendations, user feeds, citation ranking, and peer review assistant tools. In the second part of the talk I will discuss TL;DR, an extreme summarization dataset and model for scientific papers that provides a single sentence summary of an entire scientific paper. Our model uses a simple scaffolding strategy to leverage the title of papers during training and is able to achieve substantial improvements on this low-resource and challenging task.

Bio: **Arman Cohan** is a Research Scientist at the Allen Institute for AI and an affiliate Assistant Professor at University of Washington. His research primarily focuses on representation learning, language modeling and transfer learning methods in NLP, as well as their applications in the scientific and health domains. He obtained his PhD at Georgetown University in 2018 and research has been recognized with best paper award at EMNLP 2017, honorable mention at COLING 2018, and Harold N. Glassman Distinguished Doctoral Dissertation award in 2019.

